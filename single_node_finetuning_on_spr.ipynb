{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Your Chatbot on a Single Node Xeon SPR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralChat is a customizable chat framework designed to create user own chatbot within few minutes on multiple architectures. This notebook will introduce how to finetune your chatbot on the customized data on a single node Xeon SPR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install intel extension for transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: intel-extension-for-transformers in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from intel-extension-for-transformers) (23.2)\n",
      "Requirement already satisfied: numpy in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from intel-extension-for-transformers) (1.23.5)\n",
      "Requirement already satisfied: schema in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from intel-extension-for-transformers) (0.7.7)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from intel-extension-for-transformers) (6.0.1)\n",
      "Requirement already satisfied: neural-compressor in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from intel-extension-for-transformers) (2.6)\n",
      "Requirement already satisfied: transformers in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from intel-extension-for-transformers) (4.41.2)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (1.2.14)\n",
      "Requirement already satisfied: opencv-python-headless in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (4.10.0.84)\n",
      "Requirement already satisfied: pandas in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (2.2.2)\n",
      "Requirement already satisfied: Pillow in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (10.2.0)\n",
      "Requirement already satisfied: prettytable in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (3.10.0)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (9.0.0)\n",
      "Requirement already satisfied: requests in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (2.32.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (1.4.0)\n",
      "Requirement already satisfied: pycocotools in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->intel-extension-for-transformers) (2.0.8)\n",
      "Requirement already satisfied: filelock in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers->intel-extension-for-transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers->intel-extension-for-transformers) (0.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers->intel-extension-for-transformers) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers->intel-extension-for-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers->intel-extension-for-transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers->intel-extension-for-transformers) (4.66.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from deprecated>=1.2.13->neural-compressor->intel-extension-for-transformers) (1.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers->intel-extension-for-transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers->intel-extension-for-transformers) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->neural-compressor->intel-extension-for-transformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->neural-compressor->intel-extension-for-transformers) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->neural-compressor->intel-extension-for-transformers) (2023.4)\n",
      "Requirement already satisfied: wcwidth in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from prettytable->neural-compressor->intel-extension-for-transformers) (0.2.13)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pycocotools->neural-compressor->intel-extension-for-transformers) (3.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->neural-compressor->intel-extension-for-transformers) (2024.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from scikit-learn->neural-compressor->intel-extension-for-transformers) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from scikit-learn->neural-compressor->intel-extension-for-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from scikit-learn->neural-compressor->intel-extension-for-transformers) (3.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (6.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->neural-compressor->intel-extension-for-transformers) (1.16.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=2.1.0->pycocotools->neural-compressor->intel-extension-for-transformers) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'intel-extension-for-transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/intel/intel-extension-for-transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u41a5ebdbca214faa0191cc237aed1b9/intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/docs/notebooks/intel-extension-for-transformers/intel_extension_for_transformers/neural_chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (0.28.0)\n",
      "Requirement already satisfied: cchardet in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (2.1.7)\n",
      "Requirement already satisfied: einops in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (0.8.0)\n",
      "Requirement already satisfied: evaluate in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.4.2)\n",
      "Requirement already satisfied: fastapi in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.111.0)\n",
      "Requirement already satisfied: fschat==0.2.35 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.2.35)\n",
      "Requirement already satisfied: huggingface_hub in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 7)) (0.23.4)\n",
      "Requirement already satisfied: intel_extension_for_pytorch==2.3.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 8)) (2.3.0+cpu)\n",
      "Requirement already satisfied: lm-eval in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 9)) (0.4.2)\n",
      "Requirement already satisfied: neural-compressor in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 10)) (2.6)\n",
      "Requirement already satisfied: neural_speed==1.0a0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 11)) (1.0a0)\n",
      "Requirement already satisfied: numpy==1.23.5 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 12)) (1.23.5)\n",
      "Requirement already satisfied: onnx>=1.15.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 13)) (1.16.1)\n",
      "Requirement already satisfied: optimum in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 14)) (1.20.0)\n",
      "Requirement already satisfied: optimum-intel in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 15)) (1.18.0)\n",
      "Requirement already satisfied: peft==0.6.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 16)) (0.6.2)\n",
      "Requirement already satisfied: pydantic==1.10.13 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 17)) (1.10.13)\n",
      "Requirement already satisfied: python-dotenv in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 18)) (1.0.1)\n",
      "Requirement already satisfied: python-multipart in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 19)) (0.0.9)\n",
      "Requirement already satisfied: rouge_score in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 20)) (0.1.2)\n",
      "Requirement already satisfied: sacremoses in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 21)) (0.1.1)\n",
      "Requirement already satisfied: shortuuid in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 22)) (1.0.13)\n",
      "Requirement already satisfied: starlette in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 23)) (0.37.2)\n",
      "Requirement already satisfied: tensorflow>=2.13.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 24)) (2.16.2)\n",
      "Requirement already satisfied: torch==2.3.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 25)) (2.3.0)\n",
      "Requirement already satisfied: torchaudio==2.3.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 26)) (2.3.0)\n",
      "Requirement already satisfied: transformers>=4.35.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 27)) (4.41.2)\n",
      "Requirement already satisfied: transformers_stream_generator in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 28)) (0.0.5)\n",
      "Requirement already satisfied: uvicorn in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 29)) (0.30.1)\n",
      "Requirement already satisfied: vllm in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 30)) (0.2.5)\n",
      "Requirement already satisfied: yacs in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from -r requirements.txt (line 31)) (0.1.8)\n",
      "Requirement already satisfied: aiohttp in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (3.9.5)\n",
      "Requirement already satisfied: httpx in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (0.27.0)\n",
      "Requirement already satisfied: markdown2[all] in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (2.4.13)\n",
      "Requirement already satisfied: nh3 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (0.2.17)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (3.0.42)\n",
      "Requirement already satisfied: requests in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: rich>=10.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (13.7.1)\n",
      "Requirement already satisfied: tiktoken in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fschat==0.2.35->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: psutil in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from intel_extension_for_pytorch==2.3.0->-r requirements.txt (line 8)) (5.9.8)\n",
      "Requirement already satisfied: packaging in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from intel_extension_for_pytorch==2.3.0->-r requirements.txt (line 8)) (23.2)\n",
      "Requirement already satisfied: pyyaml in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from peft==0.6.2->-r requirements.txt (line 16)) (6.0.1)\n",
      "Requirement already satisfied: tqdm in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from peft==0.6.2->-r requirements.txt (line 16)) (4.66.4)\n",
      "Requirement already satisfied: safetensors in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from peft==0.6.2->-r requirements.txt (line 16)) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from pydantic==1.10.13->-r requirements.txt (line 17)) (4.12.2)\n",
      "Requirement already satisfied: filelock in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (3.15.4)\n",
      "Requirement already satisfied: sympy in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (1.12.1)\n",
      "Requirement already satisfied: networkx in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (2024.5.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from torch==2.3.0->-r requirements.txt (line 25)) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r requirements.txt (line 25)) (12.5.40)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 4)) (2.20.0)\n",
      "Requirement already satisfied: dill in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 4)) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from evaluate->-r requirements.txt (line 4)) (0.70.16)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fastapi->-r requirements.txt (line 5)) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fastapi->-r requirements.txt (line 5)) (5.10.0)\n",
      "Requirement already satisfied: orjson>=3.2.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fastapi->-r requirements.txt (line 5)) (3.10.5)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fastapi->-r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: jsonlines in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: numexpr in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (2.10.1)\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (2.13.1)\n",
      "Requirement already satisfied: pytablewriter in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (1.2.0)\n",
      "Requirement already satisfied: sacrebleu>=1.5.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (2.4.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: sqlitedict in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (2.1.0)\n",
      "Requirement already satisfied: tqdm-multiprocess in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (0.0.11)\n",
      "Requirement already satisfied: zstandard in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (0.22.0)\n",
      "Requirement already satisfied: word2number in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (1.1)\n",
      "Requirement already satisfied: more-itertools in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from lm-eval->-r requirements.txt (line 9)) (10.3.0)\n",
      "Requirement already satisfied: deprecated>=1.2.13 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (1.2.14)\n",
      "Requirement already satisfied: opencv-python-headless in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (10.2.0)\n",
      "Requirement already satisfied: prettytable in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (3.10.0)\n",
      "Requirement already satisfied: py-cpuinfo in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (9.0.0)\n",
      "Requirement already satisfied: schema in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (0.7.7)\n",
      "Requirement already satisfied: pycocotools in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from neural-compressor->-r requirements.txt (line 10)) (2.0.8)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from onnx>=1.15.0->-r requirements.txt (line 13)) (4.25.3)\n",
      "Requirement already satisfied: coloredlogs in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from optimum->-r requirements.txt (line 14)) (15.0.1)\n",
      "Requirement already satisfied: sentencepiece in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from optimum-intel->-r requirements.txt (line 15)) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from optimum-intel->-r requirements.txt (line 15)) (69.1.0)\n",
      "Requirement already satisfied: scipy in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from optimum-intel->-r requirements.txt (line 15)) (1.10.1)\n",
      "Requirement already satisfied: absl-py in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from rouge_score->-r requirements.txt (line 20)) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from rouge_score->-r requirements.txt (line 20)) (3.8.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from rouge_score->-r requirements.txt (line 20)) (1.16.0)\n",
      "Requirement already satisfied: regex in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from sacremoses->-r requirements.txt (line 21)) (2024.5.15)\n",
      "Requirement already satisfied: click in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from sacremoses->-r requirements.txt (line 21)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from sacremoses->-r requirements.txt (line 21)) (1.3.2)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from starlette->-r requirements.txt (line 23)) (4.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (3.3.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (2.4.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.37.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from transformers>=4.35.2->-r requirements.txt (line 27)) (0.19.1)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from uvicorn->-r requirements.txt (line 29)) (0.14.0)\n",
      "Requirement already satisfied: ninja in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from vllm->-r requirements.txt (line 30)) (1.11.1.1)\n",
      "Requirement already satisfied: ray>=2.5.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from vllm->-r requirements.txt (line 30)) (2.31.0)\n",
      "Requirement already satisfied: pyarrow in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from vllm->-r requirements.txt (line 30)) (16.1.0)\n",
      "Requirement already satisfied: xformers>=0.0.23 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from vllm->-r requirements.txt (line 30)) (0.0.26.post1)\n",
      "Requirement already satisfied: aioprometheus[starlette] in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from vllm->-r requirements.txt (line 30)) (23.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette->-r requirements.txt (line 23)) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette->-r requirements.txt (line 23)) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette->-r requirements.txt (line 23)) (1.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.42.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate->-r requirements.txt (line 4)) (0.6)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from email_validator>=2.0.0->fastapi->-r requirements.txt (line 5)) (2.6.1)\n",
      "Requirement already satisfied: typer>=0.12.3 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from fastapi-cli>=0.0.2->fastapi->-r requirements.txt (line 5)) (0.12.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from aiohttp->fschat==0.2.35->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from aiohttp->fschat==0.2.35->-r requirements.txt (line 6)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from aiohttp->fschat==0.2.35->-r requirements.txt (line 6)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from aiohttp->fschat==0.2.35->-r requirements.txt (line 6)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from aiohttp->fschat==0.2.35->-r requirements.txt (line 6)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from aiohttp->fschat==0.2.35->-r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: certifi in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from httpx->fschat==0.2.35->-r requirements.txt (line 6)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from httpx->fschat==0.2.35->-r requirements.txt (line 6)) (1.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from jinja2->torch==2.3.0->-r requirements.txt (line 25)) (2.1.4)\n",
      "Requirement already satisfied: namex in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.11.0)\n",
      "Requirement already satisfied: wcwidth in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.35->-r requirements.txt (line 6)) (0.2.13)\n",
      "Requirement already satisfied: jsonschema in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from ray>=2.5.1->vllm->-r requirements.txt (line 30)) (4.21.1)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from ray>=2.5.1->vllm->-r requirements.txt (line 30)) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->fschat==0.2.35->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from requests->fschat==0.2.35->-r requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.35->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from rich>=10.0.0->fschat==0.2.35->-r requirements.txt (line 6)) (2.17.2)\n",
      "Requirement already satisfied: portalocker in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval->-r requirements.txt (line 9)) (2.10.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: colorama in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval->-r requirements.txt (line 9)) (0.4.6)\n",
      "Requirement already satisfied: lxml in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from sacrebleu>=1.5.0->lm-eval->-r requirements.txt (line 9)) (5.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from scikit-learn>=0.24.1->lm-eval->-r requirements.txt (line 9)) (3.3.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.13.0->-r requirements.txt (line 24)) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.13.0->-r requirements.txt (line 24)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow>=2.13.0->-r requirements.txt (line 24)) (3.0.3)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 5)) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 5)) (0.22.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from uvicorn[standard]>=0.12.0->fastapi->-r requirements.txt (line 5)) (12.0)\n",
      "Requirement already satisfied: quantile-python>=1.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from aioprometheus[starlette]->vllm->-r requirements.txt (line 30)) (1.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from coloredlogs->optimum->-r requirements.txt (line 14)) (10.0)\n",
      "Requirement already satisfied: wavedrom in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from markdown2[all]->fschat==0.2.35->-r requirements.txt (line 6)) (2.0.3.post3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 4)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pandas->evaluate->-r requirements.txt (line 4)) (2023.4)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from pycocotools->neural-compressor->-r requirements.txt (line 10)) (3.8.2)\n",
      "Requirement already satisfied: DataProperty<2,>=1.0.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from pytablewriter->lm-eval->-r requirements.txt (line 9)) (1.0.1)\n",
      "Requirement already satisfied: mbstrdecoder<2,>=1.0.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from pytablewriter->lm-eval->-r requirements.txt (line 9)) (1.1.3)\n",
      "Requirement already satisfied: pathvalidate<4,>=2.3.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from pytablewriter->lm-eval->-r requirements.txt (line 9)) (3.2.0)\n",
      "Requirement already satisfied: tabledata<2,>=1.3.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from pytablewriter->lm-eval->-r requirements.txt (line 9)) (1.3.3)\n",
      "Requirement already satisfied: tcolorpy<1,>=0.0.5 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from pytablewriter->lm-eval->-r requirements.txt (line 9)) (0.1.6)\n",
      "Requirement already satisfied: typepy<2,>=1.3.2 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm-eval->-r requirements.txt (line 9)) (1.3.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from sympy->torch==2.3.0->-r requirements.txt (line 25)) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow>=2.13.0->-r requirements.txt (line 24)) (7.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.35->-r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->-r requirements.txt (line 10)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->-r requirements.txt (line 10)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->-r requirements.txt (line 10)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->-r requirements.txt (line 10)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->-r requirements.txt (line 10)) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from matplotlib>=2.1.0->pycocotools->neural-compressor->-r requirements.txt (line 10)) (6.1.1)\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval->-r requirements.txt (line 9)) (5.2.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->-r requirements.txt (line 5)) (1.5.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from jsonschema->ray>=2.5.1->vllm->-r requirements.txt (line 30)) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from jsonschema->ray>=2.5.1->vllm->-r requirements.txt (line 30)) (0.34.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from jsonschema->ray>=2.5.1->vllm->-r requirements.txt (line 30)) (0.18.0)\n",
      "Requirement already satisfied: svgwrite in /home/u41a5ebdbca214faa0191cc237aed1b9/.local/lib/python3.9/site-packages (from wavedrom->markdown2[all]->fschat==0.2.35->-r requirements.txt (line 6)) (1.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/intel/oneapi/intelpython/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow>=2.13.0->-r requirements.txt (line 24)) (3.17.0)\n",
      "/home/u41a5ebdbca214faa0191cc237aed1b9/intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/docs/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: huggingface-cli: command not found\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers transformers accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60083b304a3f4e02873996d90adb08e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "We select 3 kind of datasets to conduct the finetuning process for different tasks.\n",
    "\n",
    "1. Text Generation (General domain instruction): We use the [Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca) from Stanford University as the general domain dataset to fine-tune the model. This dataset is provided in the form of a JSON file, [alpaca_data.json](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json). In Alpaca, researchers have manually crafted 175 seed tasks to guide `text-davinci-003` in generating 52K instruction data for diverse tasks.\n",
    "\n",
    "2. Summarization: An English-language dataset [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail, is used for this task.\n",
    "\n",
    "3. Code Generation: To enhance code performance of LLMs (Large Language Models), we use the [theblackcat102/evol-codealpaca-v1](https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Your Chatbot\n",
    "\n",
    "We employ the [LoRA approach](https://arxiv.org/pdf/2106.09685.pdf) to finetune the LLM efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune the model on Alpaca-format dataset to conduct text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 22:32:36.490624: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-05 22:32:37.127035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-05 22:32:37.289934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-05 22:32:37.290020: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-05 22:32:37.453875: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-05 22:32:40.987370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/transformers/training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "2024-07-05 22:32:44,512 - _logger.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - WARNING - Process rank: 0, device: cpu\n",
      "distributed training: True, 16-bits training: True\n",
      "2024-07-05 22:32:44,515 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./tmp/runs/Jul05_22-32-35_idc-training-gpu-compute-23,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=True,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./tmp,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./tmp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=True,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-07-05 22:32:45,860 >> loading configuration file config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-07-05 22:32:45,868 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-05 22:32:45,990 >> loading file tokenizer.model from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-05 22:32:45,991 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-05 22:32:45,992 >> loading file special_tokens_map.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-05 22:32:45,994 >> loading file tokenizer_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-05 22:32:45,995 >> loading file tokenizer.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.json\n",
      "Using custom data configuration default-fd4ea9282b53d64f\n",
      "2024-07-05 22:32:46,472 - builder.py - datasets.builder - INFO - Using custom data configuration default-fd4ea9282b53d64f\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "2024-07-05 22:32:46,476 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-05 22:32:46,491 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-07-05 22:32:46,494 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Found cached dataset json (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "2024-07-05 22:32:46,508 - builder.py - datasets.builder - INFO - Found cached dataset json (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-07-05 22:32:46,510 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Using custom data configuration default-fd4ea9282b53d64f\n",
      "2024-07-05 22:32:46,795 - builder.py - datasets.builder - INFO - Using custom data configuration default-fd4ea9282b53d64f\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "2024-07-05 22:32:46,798 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-05 22:32:46,806 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-07-05 22:32:46,808 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Found cached dataset json (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "2024-07-05 22:32:46,817 - builder.py - datasets.builder - INFO - Found cached dataset json (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-07-05 22:32:46,819 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Using custom data configuration default-fd4ea9282b53d64f\n",
      "2024-07-05 22:32:47,076 - builder.py - datasets.builder - INFO - Using custom data configuration default-fd4ea9282b53d64f\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "2024-07-05 22:32:47,078 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-05 22:32:47,085 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-07-05 22:32:47,088 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "Found cached dataset json (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "2024-07-05 22:32:47,097 - builder.py - datasets.builder - INFO - Found cached dataset json (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "2024-07-05 22:32:47,100 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a\n",
      "[INFO|modeling_utils.py:3474] 2024-07-05 22:32:47,381 >> loading weights file model.safetensors from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-07-05 22:32:47,392 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-07-05 22:32:47,397 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643704cfe78d4276b0f83d630527809a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-07-05 22:33:26,204 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-07-05 22:33:26,206 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-07-05 22:33:26,313 >> loading configuration file generation_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-07-05 22:33:26,314 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-38615850161246bb.arrow\n",
      "2024-07-05 22:33:26,574 - arrow_dataset.py - datasets.arrow_dataset - INFO - Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-38615850161246bb.arrow\n",
      "Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-dee8e6530fc13a6f.arrow\n",
      "2024-07-05 22:33:26,590 - arrow_dataset.py - datasets.arrow_dataset - INFO - Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/json/default-fd4ea9282b53d64f/0.0.0/7483f22a71512872c377524b97484f6d20c275799bb9e7cd8fb3198178d8220a/cache-dee8e6530fc13a6f.arrow\n",
      "2024-07-05 22:33:26,593 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Using data collator of type DataCollatorForSeq2Seq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:641] 2024-07-05 22:33:26,938 >> Using cpu_amp half precision backend\n",
      "[INFO|trainer.py:2078] 2024-07-05 22:33:27,186 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-07-05 22:33:27,187 >>   Num examples = 2,940\n",
      "[INFO|trainer.py:2080] 2024-07-05 22:33:27,188 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2081] 2024-07-05 22:33:27,190 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2084] 2024-07-05 22:33:27,191 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-07-05 22:33:27,192 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2086] 2024-07-05 22:33:27,193 >>   Total optimization steps = 1,104\n",
      "[INFO|trainer.py:2087] 2024-07-05 22:33:27,196 >>   Number of trainable parameters = 4,194,304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1104' max='1104' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1104/1104 3:34:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2329] 2024-07-06 02:08:26,885 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3410] 2024-07-06 02:08:26,894 >> Saving model checkpoint to ./tmp\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-06 02:08:26,921 >> tokenizer config file saved in ./tmp/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-06 02:08:26,925 >> Special tokens file saved in ./tmp/special_tokens_map.json\n",
      "2024-07-06 02:08:26,948 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - *** Evaluate After Training***\n",
      "[INFO|trainer.py:3719] 2024-07-06 02:08:26,955 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-07-06 02:08:26,956 >>   Num examples = 30\n",
      "[INFO|trainer.py:3724] 2024-07-06 02:08:26,957 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     1.1063\n",
      "  eval_ppl                =     3.0231\n",
      "  eval_runtime            = 0:00:13.99\n",
      "  eval_samples            =         30\n",
      "  eval_samples_per_second =      2.144\n",
      "  eval_steps_per_second   =      0.286\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "\n",
    "model_args = ModelArguments(model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "data_args = DataArguments(train_file=\"alpacadata.json\", validation_split_percentage=1)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments()\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "    model_args=model_args,\n",
    "    data_args=data_args,\n",
    "    training_args=training_args,\n",
    "    finetune_args=finetune_args,\n",
    ")\n",
    "\n",
    "finetune_model(finetune_cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Finetune the model on the summarization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 13:19:06.916606: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-08 13:19:08.323295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 13:19:08.640094: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 13:19:08.640254: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 13:19:08.932555: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 13:19:11.891650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/transformers/training_args.py:1489: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "2024-07-08 13:19:14,445 - _logger.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - WARNING - Process rank: 0, device: cpu\n",
      "distributed training: True, 16-bits training: True\n",
      "2024-07-08 13:19:14,448 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./tmp/runs/Jul08_13-19-04_idc-training-gpu-compute-27,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=True,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./tmp,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./tmp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=True,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-07-08 13:19:15,356 >> loading configuration file config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-07-08 13:19:15,362 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 13:19:15,462 >> loading file tokenizer.model from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 13:19:15,463 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 13:19:15,463 >> loading file special_tokens_map.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 13:19:15,464 >> loading file tokenizer_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 13:19:15,464 >> loading file tokenizer.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.json\n",
      "Using custom data configuration default-06fd546b33afb308\n",
      "2024-07-08 13:19:15,913 - builder.py - datasets.builder - INFO - Using custom data configuration default-06fd546b33afb308\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "2024-07-08 13:19:15,914 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-08 13:19:15,921 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-08 13:19:15,922 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "2024-07-08 13:19:15,930 - builder.py - datasets.builder - INFO - Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-08 13:19:15,931 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Using custom data configuration default-06fd546b33afb308\n",
      "2024-07-08 13:19:16,200 - builder.py - datasets.builder - INFO - Using custom data configuration default-06fd546b33afb308\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "2024-07-08 13:19:16,202 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-08 13:19:16,208 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-08 13:19:16,209 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "2024-07-08 13:19:16,213 - builder.py - datasets.builder - INFO - Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-08 13:19:16,214 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Using custom data configuration default-06fd546b33afb308\n",
      "2024-07-08 13:19:16,470 - builder.py - datasets.builder - INFO - Using custom data configuration default-06fd546b33afb308\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "2024-07-08 13:19:16,471 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-08 13:19:16,475 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-08 13:19:16,476 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "2024-07-08 13:19:16,481 - builder.py - datasets.builder - INFO - Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-08 13:19:16,482 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "[INFO|modeling_utils.py:3474] 2024-07-08 13:19:16,651 >> loading weights file model.safetensors from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-07-08 13:19:16,656 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-07-08 13:19:16,659 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee79eb80cc5f430aa633f3f8be840ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-07-08 13:20:40,188 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-07-08 13:20:40,190 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-07-08 13:20:40,292 >> loading configuration file generation_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-07-08 13:20:40,294 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-85bd9b7c0608350b.arrow\n",
      "2024-07-08 13:20:40,436 - arrow_dataset.py - datasets.arrow_dataset - INFO - Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-85bd9b7c0608350b.arrow\n",
      "Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-ae0e716cb380c4c0.arrow\n",
      "2024-07-08 13:20:40,451 - arrow_dataset.py - datasets.arrow_dataset - INFO - Loading cached processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-06fd546b33afb308/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-ae0e716cb380c4c0.arrow\n",
      "2024-07-08 13:20:40,453 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Using data collator of type DataCollatorForSeq2Seq\n",
      "[INFO|trainer.py:641] 2024-07-08 13:20:40,777 >> Using cpu_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:804] 2024-07-08 13:20:41,298 >> The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: decoder_labels, decoder_input_ids, decoder_attention_mask. If decoder_labels, decoder_input_ids, decoder_attention_mask are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2078] 2024-07-08 13:20:41,337 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-07-08 13:20:41,338 >>   Num examples = 545\n",
      "[INFO|trainer.py:2080] 2024-07-08 13:20:41,338 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2081] 2024-07-08 13:20:41,339 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2084] 2024-07-08 13:20:41,339 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-07-08 13:20:41,339 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2086] 2024-07-08 13:20:41,340 >>   Total optimization steps = 207\n",
      "[INFO|trainer.py:2087] 2024-07-08 13:20:41,343 >>   Number of trainable parameters = 4,194,304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='207' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [207/207 3:07:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2329] 2024-07-08 16:30:13,397 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3410] 2024-07-08 16:30:13,403 >> Saving model checkpoint to ./tmp\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-08 16:30:13,460 >> tokenizer config file saved in ./tmp/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-08 16:30:13,477 >> Special tokens file saved in ./tmp/special_tokens_map.json\n",
      "2024-07-08 16:30:13,505 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - *** Evaluate After Training***\n",
      "[INFO|trainer.py:804] 2024-07-08 16:30:13,507 >> The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: decoder_labels, decoder_input_ids, decoder_attention_mask. If decoder_labels, decoder_input_ids, decoder_attention_mask are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:3719] 2024-07-08 16:30:13,517 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-07-08 16:30:13,518 >>   Num examples = 6\n",
      "[INFO|trainer.py:3724] 2024-07-08 16:30:13,518 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =      1.113\n",
      "  eval_ppl                =     3.0435\n",
      "  eval_runtime            = 0:00:12.09\n",
      "  eval_samples            =          6\n",
      "  eval_samples_per_second =      0.496\n",
      "  eval_steps_per_second   =      0.083\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710d366e2fd84b1f927310a36119464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 16:33:12,750 - chatbot.py - intel_extension_for_transformers.neural_chat.chatbot - ERROR - Exception: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/home/u41a5ebdbca214faa0191cc237aed1b9/nltk_data'\n",
      "    - '/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/nltk_data'\n",
      "    - '/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/share/nltk_data'\n",
      "    - '/home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n",
      "2024-07-08 16:33:12,751 - error_utils.py - intel_extension_for_transformers.neural_chat.utils.error_utils - ERROR - neuralchat error: LORA finetuning failed\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "model_args = ModelArguments(model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "data_args = DataArguments(train_file=\"cnndailymail.csv\", validation_split_percentage=1)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "finetune_args = FinetuningArguments(task='summarization')\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "            model_args=model_args,\n",
    "            data_args=data_args,\n",
    "            training_args=training_args,\n",
    "            finetune_args=finetune_args,\n",
    "        )\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune the model on the code generation task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2052] 2024-07-07 23:30:43,899 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1727] 2024-07-07 23:30:43,901 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[WARNING|_logger.py:72] 2024-07-07 23:30:43,911 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "[INFO|training_args.py:2052] 2024-07-07 23:30:43,913 >> PyTorch: setting up devices\n",
      "2024-07-07 23:30:43,914 - _logger.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - WARNING - Process rank: 0, device: cpu\n",
      "distributed training: True, 16-bits training: True\n",
      "2024-07-07 23:30:43,916 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=False,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=no,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./tmp/runs/Jul07_23-30-43_idc-training-gpu-compute-26,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=True,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./tmp,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=8,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./tmp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=True,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "[INFO|configuration_utils.py:733] 2024-07-07 23:30:44,038 >> loading configuration file config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-07-07 23:30:44,040 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-07 23:30:44,146 >> loading file tokenizer.model from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-07 23:30:44,147 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-07 23:30:44,147 >> loading file special_tokens_map.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-07 23:30:44,148 >> loading file tokenizer_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-07 23:30:44,148 >> loading file tokenizer.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.json\n",
      "Using custom data configuration default-c98437dc1a94c717\n",
      "2024-07-07 23:30:44,557 - builder.py - datasets.builder - INFO - Using custom data configuration default-c98437dc1a94c717\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "2024-07-07 23:30:44,559 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "Generating dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "2024-07-07 23:30:44,569 - builder.py - datasets.builder - INFO - Generating dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "Downloading and preparing dataset csv/default to /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52...\n",
      "2024-07-07 23:30:44,571 - builder.py - datasets.builder - INFO - Downloading and preparing dataset csv/default to /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52...\n",
      "Downloading took 0.0 min\n",
      "2024-07-07 23:30:44,574 - download_manager.py - datasets.download.download_manager - INFO - Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "2024-07-07 23:30:44,582 - download_manager.py - datasets.download.download_manager - INFO - Checksum Computation took 0.0 min\n",
      "Generating train split\n",
      "2024-07-07 23:30:44,586 - builder.py - datasets.builder - INFO - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9aad14442245eb9423f1e1e4b8a449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify splits sizes.\n",
      "2024-07-07 23:30:44,622 - info_utils.py - datasets.utils.info_utils - INFO - Unable to verify splits sizes.\n",
      "Dataset csv downloaded and prepared to /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52. Subsequent calls will reuse this data.\n",
      "2024-07-07 23:30:44,627 - builder.py - datasets.builder - INFO - Dataset csv downloaded and prepared to /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52. Subsequent calls will reuse this data.\n",
      "Using custom data configuration default-c98437dc1a94c717\n",
      "2024-07-07 23:30:44,878 - builder.py - datasets.builder - INFO - Using custom data configuration default-c98437dc1a94c717\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "2024-07-07 23:30:44,879 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-07 23:30:44,883 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-07 23:30:44,884 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "2024-07-07 23:30:44,889 - builder.py - datasets.builder - INFO - Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-07 23:30:44,890 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Using custom data configuration default-c98437dc1a94c717\n",
      "2024-07-07 23:30:45,138 - builder.py - datasets.builder - INFO - Using custom data configuration default-c98437dc1a94c717\n",
      "Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "2024-07-07 23:30:45,139 - info.py - datasets.info - INFO - Loading Dataset Infos from /home/u41a5ebdbca214faa0191cc237aed1b9/.conda/envs/itrex-l/lib/python3.10/site-packages/datasets/packaged_modules/csv\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "2024-07-07 23:30:45,143 - builder.py - datasets.builder - INFO - Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-07 23:30:45,144 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "2024-07-07 23:30:45,148 - builder.py - datasets.builder - INFO - Found cached dataset csv (/home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52)\n",
      "Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "2024-07-07 23:30:45,149 - info.py - datasets.info - INFO - Loading Dataset info from /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52\n",
      "[INFO|modeling_utils.py:3474] 2024-07-07 23:30:45,161 >> loading weights file model.safetensors from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-07-07 23:30:45,164 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:962] 2024-07-07 23:30:45,166 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f265692e0644e2f828a49fbd9ee5225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-07-07 23:30:49,133 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-07-07 23:30:49,134 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-07-07 23:30:49,240 >> loading configuration file generation_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-07-07 23:30:49,242 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0663ceb034c2495187c882e6db2ddb92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-7ead664883c9cfe2.arrow\n",
      "2024-07-07 23:30:52,607 - arrow_dataset.py - datasets.arrow_dataset - INFO - Caching processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-7ead664883c9cfe2.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab11bc09bebc418bbc8b3f6a672eea3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-7b8dda294857f4df.arrow\n",
      "2024-07-07 23:30:52,817 - arrow_dataset.py - datasets.arrow_dataset - INFO - Caching processed dataset at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/datasets/csv/default-c98437dc1a94c717/0.0.0/9ea1179385ff7ad1e756d327ffccaa3b801175702a2d91528226ba2c66873f52/cache-7b8dda294857f4df.arrow\n",
      "2024-07-07 23:30:52,835 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - Using data collator of type DataCollatorForSeq2Seq\n",
      "[INFO|trainer.py:641] 2024-07-07 23:30:53,007 >> Using cpu_amp half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2078] 2024-07-07 23:30:53,544 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-07-07 23:30:53,544 >>   Num examples = 394\n",
      "[INFO|trainer.py:2080] 2024-07-07 23:30:53,545 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2081] 2024-07-07 23:30:53,545 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2084] 2024-07-07 23:30:53,546 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2085] 2024-07-07 23:30:53,546 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2086] 2024-07-07 23:30:53,546 >>   Total optimization steps = 150\n",
      "[INFO|trainer.py:2087] 2024-07-07 23:30:53,550 >>   Number of trainable parameters = 4,194,304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 2:00:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2329] 2024-07-08 01:32:14,301 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:3410] 2024-07-08 01:32:14,309 >> Saving model checkpoint to ./tmp\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-08 01:32:14,380 >> tokenizer config file saved in ./tmp/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-08 01:32:14,394 >> Special tokens file saved in ./tmp/special_tokens_map.json\n",
      "2024-07-08 01:32:14,421 - finetuning.py - intel_extension_for_transformers.transformers.llm.finetuning.finetuning - INFO - *** Evaluate After Training***\n",
      "[INFO|trainer.py:3719] 2024-07-08 01:32:14,434 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-07-08 01:32:14,435 >>   Num examples = 4\n",
      "[INFO|trainer.py:3724] 2024-07-08 01:32:14,436 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =     1.0583\n",
      "  eval_ppl                =     2.8814\n",
      "  eval_runtime            = 0:00:11.66\n",
      "  eval_samples            =          4\n",
      "  eval_samples_per_second =      0.343\n",
      "  eval_steps_per_second   =      0.086\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "model_args = ModelArguments(model_name_or_path=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "data_args = DataArguments(train_file=\"evolcodealpaca.csv\",validation_split_percentage=1)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./tmp',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "finetune_args = FinetuningArguments(task='code-generation')\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "            model_args=model_args,\n",
    "            data_args=data_args,\n",
    "            training_args=training_args,\n",
    "            finetune_args=finetune_args,\n",
    "        )\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-07-08 16:39:46,432 >> loading configuration file config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/config.json\n",
      "[INFO|configuration_utils.py:796] 2024-07-08 16:39:46,435 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3474] 2024-07-08 16:39:46,439 >> loading weights file model.safetensors from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/model.safetensors.index.json\n",
      "[INFO|configuration_utils.py:962] 2024-07-08 16:39:46,443 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31424ce4e1034866a43f134361f45035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-07-08 16:40:00,769 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-07-08 16:40:00,771 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-07-08 16:40:00,876 >> loading configuration file generation_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-07-08 16:40:00,878 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 16:40:00,993 >> loading file tokenizer.model from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 16:40:00,994 >> loading file tokenizer.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 16:40:00,995 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 16:40:00,995 >> loading file special_tokens_map.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-08 16:40:00,996 >> loading file tokenizer_config.json from cache at /home/u41a5ebdbca214faa0191cc237aed1b9/.cache/huggingface/hub/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot response: The script currently has a bug where it attempts to print an object that is outside the bounds of the list. Fix this error and modify the script to use 'while' instead of 'for' loop. Ensure your script correctly handles empty lists.  (10 points)\n",
      "\n",
      "Here is the current script:\n",
      "```\n",
      "def print_list(list):\n",
      "  for element in list:  # Bug is in this line\n",
      "    print(element) \n",
      "  \n",
      "list = [1, 2,  3,4,5]\n",
      "print_List( list )\n",
      " ```\n",
      "Please modify it to fix the bug and use a 'whiles' loops instead.\n",
      "Thank you!\n",
      "(15 points total)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def interact_with_chatbot(query):\n",
    "    inputs = tokenizer.encode(query, return_tensors=\"pt\")\n",
    "    response = model.generate(inputs, max_length=500, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    return tokenizer.decode(response[0], skip_special_tokens=True)\n",
    "\n",
    "query = \"The script currently has a bug where it attempts to print an object that is outside the bounds of the list. Fix this error and modify the script to use 'while' instead of 'for' loop. Ensure your script correctly handles empty lists. \"\n",
    "response = interact_with_chatbot(query)\n",
    "print(\"Chatbot response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-chat",
   "language": "python",
   "name": "neural-chat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
